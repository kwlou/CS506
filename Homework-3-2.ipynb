{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Number of Comments for Blogs ##\n",
    "\n",
    "In this assignment, we will work on blog posts. The deadline is ** Nov. 19 5pm **. The goal is to determine the number of comments a post will receive in the next 24 hours. You can find the details of the dataset [here](https://drive.google.com/a/bu.edu/file/d/19p7QBjH2Ai37MJcixiQ5UKlC0kw9l2Yd/view?usp=sharing). The data are stored in .csv files and each row of the dataset corresponds to a distinct blog (data instance). \n",
    "\n",
    "For information regarding the dataset and the features included in it please refer to the included README.md file.\n",
    "\n",
    "As learned in class most algorithms can only handle numeric values so we provided a dataset containing numeric values. The algorithms you are going to evaluate are the following: 1) Linear Regression, 2) Logistic Regression, 3) KNN, 4) Decision Tree Classifiers. Tasks 1 and 2 are related to regression to analyze this information, while tasks 3 and 4 are related to classification.\n",
    "\n",
    "Relevant Papers/citations:\n",
    "1) Buza, K. (2014). Feedback Prediction for Blogs. In Data Analysis, Machine Learning and Knowledge Discovery (pp. 145-152). Springer International Publishing.\n",
    "2) BlogFeedback Data Set UCI Machine Learning Repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "\n",
    "First, we will fit a linear regression model that predicts the number of comments a post will receive. Use the model to analyze the important factors that decide the number of comments of a blog. \n",
    "\n",
    "1. Report the results of your linear regression model (error, factors, e.t.c.) for the test datasets a) blogData_test-2012.02.csv b) blogData_test-2012.03.csv ** (10 points) **\n",
    "2. Provide a brief report explaining a) how did you address overfitting, b) interpretation of the linear regression model results, c) compare the results for the two test datasets and mention any interesting observations ** (5 points) **\n",
    "\n",
    "Total: ** (15 points) **\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import neighbors, datasets\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import sklearn.datasets as ds\n",
    "from sklearn import linear_model, datasets\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>...</th>\n",
       "      <th>48</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>58</th>\n",
       "      <th>60</th>\n",
       "      <th>280</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40.30467</td>\n",
       "      <td>53.845657</td>\n",
       "      <td>401.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.52416</td>\n",
       "      <td>32.44188</td>\n",
       "      <td>377.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>14.044226</td>\n",
       "      <td>32.615417</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40.30467</td>\n",
       "      <td>53.845657</td>\n",
       "      <td>401.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.52416</td>\n",
       "      <td>32.44188</td>\n",
       "      <td>377.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>14.044226</td>\n",
       "      <td>32.615417</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40.30467</td>\n",
       "      <td>53.845657</td>\n",
       "      <td>401.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.52416</td>\n",
       "      <td>32.44188</td>\n",
       "      <td>377.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>14.044226</td>\n",
       "      <td>32.615417</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40.30467</td>\n",
       "      <td>53.845657</td>\n",
       "      <td>401.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.52416</td>\n",
       "      <td>32.44188</td>\n",
       "      <td>377.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>14.044226</td>\n",
       "      <td>32.615417</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40.30467</td>\n",
       "      <td>53.845657</td>\n",
       "      <td>401.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.52416</td>\n",
       "      <td>32.44188</td>\n",
       "      <td>377.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>14.044226</td>\n",
       "      <td>32.615417</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0          1      3     4         5         6      8    9    \\\n",
       "0  40.30467  53.845657  401.0  15.0  15.52416  32.44188  377.0  3.0   \n",
       "1  40.30467  53.845657  401.0  15.0  15.52416  32.44188  377.0  3.0   \n",
       "2  40.30467  53.845657  401.0  15.0  15.52416  32.44188  377.0  3.0   \n",
       "3  40.30467  53.845657  401.0  15.0  15.52416  32.44188  377.0  3.0   \n",
       "4  40.30467  53.845657  401.0  15.0  15.52416  32.44188  377.0  3.0   \n",
       "\n",
       "         10         11   ...   48   50   51   53   54   55   56   58    60   \\\n",
       "0  14.044226  32.615417  ...   9.0  2.0  2.0  2.0  2.0  0.0  0.0  0.0  10.0   \n",
       "1  14.044226  32.615417  ...   9.0  6.0  2.0  5.0 -2.0  0.0  0.0  0.0  35.0   \n",
       "2  14.044226  32.615417  ...   9.0  6.0  2.0  5.0 -2.0  0.0  0.0  0.0  35.0   \n",
       "3  14.044226  32.615417  ...   9.0  2.0  2.0  2.0  2.0  0.0  0.0  0.0  10.0   \n",
       "4  14.044226  32.615417  ...   9.0  3.0  1.0  2.0 -1.0  0.0  0.0  0.0  34.0   \n",
       "\n",
       "    280  \n",
       "0   1.0  \n",
       "1   0.0  \n",
       "2   0.0  \n",
       "3   1.0  \n",
       "4  27.0  \n",
       "\n",
       "[5 rows x 48 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "traindata = pd.read_csv(\"blogData_train.csv\", header=-1)\n",
    "data02 = pd.read_csv(\"blogData_test-2012.02.csv\", header=-1)\n",
    "data03 = pd.read_csv(\"blogData_test-2012.03.csv\", header=-1)\n",
    "\n",
    "correlation = np.corrcoef(traindata, rowvar=0)\n",
    "#Delete columns that do not make the cutoff\n",
    "i = 0\n",
    "goodfeatures={}\n",
    "for feature in correlation:\n",
    "    #NOTE: 0.1 is the best cutoff for Linear Regression, gives us the highest accuracy store\n",
    "    if(feature[280] > 0.15 or feature[280] < -0.15): \n",
    "        goodfeatures[i] = feature[280]\n",
    "    else:\n",
    "        traindata = traindata.drop([i], axis=1)\n",
    "        data02 = data02.drop([i], axis=1)\n",
    "    i += 1\n",
    "\n",
    "ft = []\n",
    "for feature in goodfeatures:\n",
    "    ft.append(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(traindata[ft],traindata[280])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm = LinearRegression()\n",
    "lm.fit(traindata[ft],traindata[280])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>...</th>\n",
       "      <th>48</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>58</th>\n",
       "      <th>60</th>\n",
       "      <th>280</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40.30467</td>\n",
       "      <td>53.845657</td>\n",
       "      <td>401.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.52416</td>\n",
       "      <td>32.44188</td>\n",
       "      <td>377.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>14.044226</td>\n",
       "      <td>32.615417</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40.30467</td>\n",
       "      <td>53.845657</td>\n",
       "      <td>401.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.52416</td>\n",
       "      <td>32.44188</td>\n",
       "      <td>377.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>14.044226</td>\n",
       "      <td>32.615417</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40.30467</td>\n",
       "      <td>53.845657</td>\n",
       "      <td>401.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.52416</td>\n",
       "      <td>32.44188</td>\n",
       "      <td>377.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>14.044226</td>\n",
       "      <td>32.615417</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40.30467</td>\n",
       "      <td>53.845657</td>\n",
       "      <td>401.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.52416</td>\n",
       "      <td>32.44188</td>\n",
       "      <td>377.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>14.044226</td>\n",
       "      <td>32.615417</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40.30467</td>\n",
       "      <td>53.845657</td>\n",
       "      <td>401.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.52416</td>\n",
       "      <td>32.44188</td>\n",
       "      <td>377.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>14.044226</td>\n",
       "      <td>32.615417</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0          1      3     4         5         6      8    9    \\\n",
       "0  40.30467  53.845657  401.0  15.0  15.52416  32.44188  377.0  3.0   \n",
       "1  40.30467  53.845657  401.0  15.0  15.52416  32.44188  377.0  3.0   \n",
       "2  40.30467  53.845657  401.0  15.0  15.52416  32.44188  377.0  3.0   \n",
       "3  40.30467  53.845657  401.0  15.0  15.52416  32.44188  377.0  3.0   \n",
       "4  40.30467  53.845657  401.0  15.0  15.52416  32.44188  377.0  3.0   \n",
       "\n",
       "         10         11   ...   48   50   51   53   54   55   56   58    60   \\\n",
       "0  14.044226  32.615417  ...   9.0  2.0  2.0  2.0  2.0  0.0  0.0  0.0  10.0   \n",
       "1  14.044226  32.615417  ...   9.0  6.0  2.0  5.0 -2.0  0.0  0.0  0.0  35.0   \n",
       "2  14.044226  32.615417  ...   9.0  6.0  2.0  5.0 -2.0  0.0  0.0  0.0  35.0   \n",
       "3  14.044226  32.615417  ...   9.0  2.0  2.0  2.0  2.0  0.0  0.0  0.0  10.0   \n",
       "4  14.044226  32.615417  ...   9.0  3.0  1.0  2.0 -1.0  0.0  0.0  0.0  34.0   \n",
       "\n",
       "    280  \n",
       "0   1.0  \n",
       "1   0.0  \n",
       "2   0.0  \n",
       "3   1.0  \n",
       "4  27.0  \n",
       "\n",
       "[5 rows x 48 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "\n",
    "Next, we will fit a logistic regression model that decides if a blog post is popular or not. In order to do so, note that you need to define a binary attribute on which you can fit the logistic regression model. As before, use the model to analyze the data.\n",
    "\n",
    "1. Report the results of your linear regression model (error, factors, e.t.c.) for the test datasets a) blogData_test-2012.02.csv b) blogData_test-2012.03.csv ** (10 points) **\n",
    "2. Provide a brief report explaining a) how did you define the binary attribute, b) interpretation of the logistic regression model results, c) compare the results for the two test datasets and mention any interesting observations ** (5 points) **\n",
    "\n",
    "Total: ** (15 points) **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: \n",
      " [ 938.23786125]\n",
      "Mean squared error: 2548.07\n",
      "Variance score: 0.47\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcwAAAE5CAYAAAAdhBAsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAwQmAAAGCEwAAAz8P+15W+iCm6BOAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x27a98f176d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Use only one feature\n",
    "diabetes_X = diabetes.data[:, np.newaxis, 2]\n",
    "\n",
    "# Split the data into training/testing sets\n",
    "diabetes_X_train = diabetes_X[:-20]\n",
    "diabetes_X_test = diabetes_X[-20:]\n",
    "\n",
    "# Split the targets into training/testing sets\n",
    "diabetes_y_train = diabetes.target[:-20]\n",
    "diabetes_y_test = diabetes.target[-20:]\n",
    "\n",
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "regr.fit(diabetes_X_train, diabetes_y_train)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "diabetes_y_pred = regr.predict(diabetes_X_test)\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients: \\n', regr.coef_)\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(test['Score'], diabetes_y_pred))\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score: %.2f' % r2_score(diabetes_y_test, diabetes_y_pred))\n",
    "\n",
    "# Plot outputs\n",
    "plt.scatter(diabetes_X_test, diabetes_y_test,  color='black')\n",
    "plt.plot(diabetes_X_test, diabetes_y_pred, color='blue', linewidth=3)\n",
    "\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "\n",
    "Now, we are going to use the K-Nearest Neighbors Classifier to decide if a blog post is popular or not. Please use the same popularity definition as in task 2, for this task. KNN is an instant-based classification which simply stores instances of the training data. Then, classification is computed from a majority vote of the nearest neighbors of each point.\n",
    "\n",
    "1. Report the accuracy of your prediction for the test datasets a) blogData_test-2012.02.csv b) blogData_test-2012.03.csv ** (10 points) **\n",
    "2. Provide a brief report explaining a) how did you define the nearest neighbors, b) interpretation of the KNN results, c) compare the results for the two test datasets and mention any interesting observations (5 points)\n",
    "\n",
    "Total: ** (15 points) **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4\n",
    "\n",
    "Finally, we are going to use the Decision Trees Classifier to decide if a blog post is popular or not. Please use the same popularity definition as in tasks 2,3, for this task. In order to construct a Decision Tree you will need to discretize some of the attributes.\n",
    "\n",
    "1. Report the accuracy of your prediction for the test datasets a) blogData_test-2012.02.csv b) blogData_test-2012.03.csv ** (10 points) **\n",
    "2. Provide a brief report explaining a) which attributes did you discretize and how, b) interpretation of the Decision Tree Classifier results, c) compare the results for the two test datasets and mention any interesting observations (5 points)\n",
    "\n",
    "Total: ** (15 points) **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
